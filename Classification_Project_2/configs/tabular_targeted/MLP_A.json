{
  "hidden_layer_sizes": [256, 128],
  "activation": "relu",
  "alpha": 0.001,
  "batch_size": 128,
  "learning_rate_init": 0.001,
  "max_iter": 200,
  "early_stopping": true,
  "validation_fraction": 0.15,
  "n_iter_no_change": 15,
  "random_state": 42
}
